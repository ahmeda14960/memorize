# GPU-based config for memorization evaluation
# Simplified version of the original levanter config

# ---------------------------------------------------------------------------
# Input text ----------------------------------------------------------------
# ---------------------------------------------------------------------------
book_title: "gatsby"
txt_path: "books/gatsby.txt"

# How many tokens are taken as the prompt (first half of window).
prompt_tokens: 400

# Slide the character cursor by this many chars between windows.
cursor_inc_chars: 10

# slice length
slice_length: 2000

# chunk size  
chunk_size: 500

# ---------------------------------------------------------------------------
# Tokenizer ------------------------------------------------------------------
# ---------------------------------------------------------------------------
tokenizer_name: openai/gpt-oss-120b

dtype: "bfloat16"

# ---------------------------------------------------------------------------
# Model ----------------------------------------------------------------------
# ---------------------------------------------------------------------------
# Path to HuggingFace model (local or HF hub)
initialize_from_hf: "openai/gpt-oss-120b"

# ---------------------------------------------------------------------------
# Evaluation settings -------------------------------------------------------
# ---------------------------------------------------------------------------
# works with 4 H100 GPUs
eval_batch_size: 16  # Reduced for 70B model

# Output paths
plot_path: "bar_plot_char_max_pz_gptoss_gpu.png"
histogram_path: "pz_distribution_histogram_gptoss_gpu.png"
pz_threshold: 0.0001

# Uncomment for quick debug (evaluates only first N suffix windows)
# max_examples: 100

# Debug mode - enables verbose token printing
debug: false